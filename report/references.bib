@misc{misc_magic_gamma_telescope_159,
  author       = {Bock,R.},
  title        = {{MAGIC Gamma Telescope}},
  year         = {2007},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C52C8B}
}

@online{ImbalancedlearnDocumentationVersion,
  title = {Imbalanced-Learn Documentation — {{Version}} 0.12.2},
  url = {https://imbalanced-learn.org/stable/},
  urldate = {2024-05-19},
  file = {C:\Users\user\Zotero\storage\FJDEA4AS\stable.html}
}

@inreference{KnearestNeighborsAlgorithm2024,
  title = {\emph{K}-Nearest Neighbors Algorithm},
  booktitle = {Wikipedia},
  date = {2024-03-07T11:27:26Z},
  url = {https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&oldid=1212348037},
  urldate = {2024-05-19},
  abstract = {In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression: In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor. k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically. Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.},
  langid = {english},
  annotation = {Page Version ID: 1212348037},
  file = {C:\Users\user\Zotero\storage\BJ8ZNY28\K-nearest_neighbors_algorithm.html}
}

@inreference{LogisticRegression2024,
  title = {Logistic Regression},
  booktitle = {Wikipedia},
  date = {2024-04-27T21:36:44Z},
  url = {https://en.wikipedia.org/w/index.php?title=Logistic_regression&oldid=1221099910},
  urldate = {2024-05-19},
  abstract = {In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled "0" and "1", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example. Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the "simplest" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy. The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined "logit"; see § History.},
  langid = {english},
  annotation = {Page Version ID: 1221099910},
  file = {C:\Users\user\Zotero\storage\XWE3WU6T\Logistic_regression.html}
}

@online{MatplotlibVisualizationPython,
  title = {Matplotlib — {{Visualization}} with {{Python}}},
  url = {https://matplotlib.org/},
  urldate = {2024-05-19},
  file = {C:\Users\user\Zotero\storage\QGIAUVXR\matplotlib.org.html}
}

@inreference{NaiveBayesClassifier2024,
  title = {Naive {{Bayes}} Classifier},
  booktitle = {Wikipedia},
  date = {2024-03-15T23:21:19Z},
  url = {https://en.wikipedia.org/w/index.php?title=Naive_Bayes_classifier&oldid=1213925713},
  urldate = {2024-05-19},
  abstract = {In statistics, naive Bayes classifiers are a family of linear "probabilistic classifiers" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,:{$\mkern1mu$}718{$\mkern1mu$} which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.},
  langid = {english},
  annotation = {Page Version ID: 1213925713},
  file = {C:\Users\user\Zotero\storage\CTP6ABWV\Naive_Bayes_classifier.html}
}

@online{NumPy,
  title = {{{NumPy}} -},
  url = {https://numpy.org/},
  urldate = {2024-05-19},
  file = {C:\Users\user\Zotero\storage\B5JRHJK8\numpy.org.html}
}

@online{PandasPythonData,
  title = {Pandas - {{Python Data Analysis Library}}},
  url = {https://pandas.pydata.org/},
  urldate = {2024-05-19},
  file = {C:\Users\user\Zotero\storage\85V86D7P\pandas.pydata.org.html}
}

@online{ScikitlearnMachineLearning,
  title = {Scikit-Learn: Machine Learning in {{Python}} — Scikit-Learn 1.4.2 Documentation},
  url = {https://scikit-learn.org/stable/},
  urldate = {2024-05-19},
  file = {C:\Users\user\Zotero\storage\BC9MG3GG\stable.html}
}

@inreference{SupportVectorMachine2024,
  title = {Support Vector Machine},
  booktitle = {Wikipedia},
  date = {2024-05-16T13:24:48Z},
  url = {https://en.wikipedia.org/w/index.php?title=Support_vector_machine&oldid=1224136232},
  urldate = {2024-05-19},
  abstract = {In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT\&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).  In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes                         ϵ                 \{\textbackslash displaystyle \textbackslash epsilon \}    -sensitive. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters.  The popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.},
  langid = {english},
  annotation = {Page Version ID: 1224136232}
}
